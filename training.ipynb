{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd346dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lr/Documents/github/stick-it-ai/venv/lib64/python3.13/site-packages/torchreid/reid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchreid\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98278ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "\n",
    "def resize_images():\n",
    "    # Directory containing the images\n",
    "    image_dir = 'annotated_images/pins'\n",
    "\n",
    "    # Directory to save resized images (optional)\n",
    "    resized_dir = 'reid-data/annotated_images/pins'\n",
    "    os.makedirs(resized_dir, exist_ok=True)\n",
    "\n",
    "    # Resize dimensions\n",
    "    width = 128\n",
    "    height = int(width * 4 / 3)  # Maintain 3:4 horizontal aspect ratio\n",
    "\n",
    "    # Iterate through all images in the directory\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):  # Check for image files\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            with Image.open(img_path) as img:\n",
    "                # Correct orientation based on EXIF data\n",
    "                img = ImageOps.exif_transpose(img)\n",
    "                # Resize the image\n",
    "                resized_img = img.resize((width, height))\n",
    "                # Save the resized image\n",
    "                resized_img.save(os.path.join(resized_dir, filename))\n",
    "\n",
    "    print(\"All images have been resized and saved with correct orientation.\")\n",
    "# resize_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d96fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import os.path as osp\n",
    "class StickerDataset(torchreid.data.ImageDataset):\n",
    "    dataset_dir = ''\n",
    "    csv_file = 'annotated_data.csv'\n",
    "\n",
    "    def __init__(self, root='', **kwargs):\n",
    "        self.root = osp.abspath(osp.expanduser(root))\n",
    "        csv_path = osp.join(self.root, self.csv_file)\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Generate camid mapping\n",
    "        user_ids = df['user_id'].unique()\n",
    "        self.user_id_to_camid = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "\n",
    "        data = []\n",
    "        for _, row in df.iterrows():\n",
    "            img_path = row['image_path']\n",
    "            if not osp.isabs(img_path):\n",
    "                img_path = osp.join(self.root, img_path)\n",
    "            group_id = row['group_id']\n",
    "            group_name = row['group_name']\n",
    "            camid = self.user_id_to_camid[row['user_id']]\n",
    "            data.append((img_path, group_id, camid, group_name))\n",
    "\n",
    "        pid_dict = defaultdict(list)\n",
    "        for img_path, group_id, camid, group_name in data:\n",
    "            pid_dict[group_id].append((img_path, group_id, camid, group_name))\n",
    "\n",
    "        train, query, gallery = [], [], []\n",
    "        random.seed(42)\n",
    "        for group_id, items in pid_dict.items():\n",
    "            if len(items) > 2:\n",
    "                random.shuffle(items)\n",
    "                query_candidate = None\n",
    "                gallery_candidate = None\n",
    "                for item in items:\n",
    "                    if query_candidate is None:\n",
    "                        query_candidate = item\n",
    "                    elif gallery_candidate is None and item[2] != query_candidate[2]:\n",
    "                        gallery_candidate = item\n",
    "                        break\n",
    "                if query_candidate and gallery_candidate:\n",
    "                    query.append(query_candidate)\n",
    "                    gallery.append(gallery_candidate)\n",
    "                    train.extend([item for item in items if item not in [query_candidate, gallery_candidate]])\n",
    "                else:\n",
    "                    query.append(items[0])\n",
    "                    train.extend(items[1:])\n",
    "        \n",
    "        all_data = train + query + gallery\n",
    "        all_group_ids = sorted({item[1] for item in all_data})\n",
    "        self.group_id_to_pid = {gid: idx for idx, gid in enumerate(all_group_ids)}\n",
    "\n",
    "        # Map group_name to pid\n",
    "        self.pid_to_group_name = {self.group_id_to_pid[gid]: next(item[3] for item in all_data if item[1] == gid) for gid in all_group_ids}\n",
    "\n",
    "        def map_pid(data):\n",
    "            return [(img_path, self.group_id_to_pid[group_id], camid) for img_path, group_id, camid, group_name in data]\n",
    "\n",
    "        train = map_pid(train)\n",
    "        query = map_pid(query)\n",
    "        gallery = map_pid(gallery)\n",
    "\n",
    "        super(StickerDataset, self).__init__(train, query, gallery, **kwargs)\n",
    "\n",
    "torchreid.data.register_image_dataset('sticker_dataset', dataset=StickerDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e99aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train transforms ...\n",
      "+ resize to 256x128\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "Building test transforms ...\n",
      "+ resize to 256x128\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "=> Loading train (source) dataset\n",
      "=> Loaded StickerDataset\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |    42 |     6494 |       300\n",
      "  query    |    42 |       42 |        34\n",
      "  gallery  |    30 |       30 |        25\n",
      "  ----------------------------------------\n",
      "=> Loading test (target) dataset\n",
      "=> Loaded StickerDataset\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |    42 |     6494 |       300\n",
      "  query    |    42 |       42 |        34\n",
      "  gallery  |    30 |       30 |        25\n",
      "  ----------------------------------------\n",
      "\n",
      "\n",
      "  **************** Summary ****************\n",
      "  source            : ['sticker_dataset']\n",
      "  # source datasets : 1\n",
      "  # source ids      : 42\n",
      "  # source images   : 6494\n",
      "  # source cameras  : 300\n",
      "  target            : ['sticker_dataset']\n",
      "  *****************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datamanager = torchreid.data.ImageDataManager(\n",
    "    root='reid-data',\n",
    "    sources='sticker_dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c27ad03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loaded StickerDataset\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |    42 |     6494 |       300\n",
      "  query    |    42 |       42 |        34\n",
      "  gallery  |    30 |       30 |        25\n",
      "  ----------------------------------------\n",
      "Train List:\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/ee7d6fc5-1c76-43fa-9af7-78c2a2cca343.png', 24, 71, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/68d1a874-7714-4b48-b558-55f5605a3be3.png', 24, 4, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/353d7785-7222-4b8e-9e59-f2a6606f48d7.png', 24, 30, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/3fe54f67-5465-4565-ba11-3559845f967b.png', 24, 300, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/9197b74f-a3b8-44c1-909d-4b89cd636add.png', 24, 30, 0)\n",
      "Query List:\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/c768092b-37bb-473a-84ef-be7a35147bd6.png', 24, 62, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/82a6768d-1840-479f-bbf4-bd79ae54ce18.png', 37, 84, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/b392e94f-ab33-4f31-b641-0e2234dec0f1.png', 4, 1, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/2d1ffaf2-e787-4fa6-ad91-65995fd79e70.png', 16, 20, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/ccda3069-f49c-4afb-b4a6-163a5c5cf86e.png', 23, 11, 0)\n",
      "Gallery List:\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/ce4d1cad-6777-42d7-9bb1-8aba15454e08.png', 24, 0, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/0ae7fed8-2103-4483-8443-a364a1b9fc56.png', 37, 79, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/a2bfc74c-1b26-420b-a1a9-2e35e4c8870b.png', 4, 49, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/143c44c5-9d6a-4b8a-9133-063eeda86e35.png', 16, 11, 0)\n",
      "('/home/lr/Documents/github/stick-it-ai/annotated_images/pins/df89fd24-0f85-46c9-81fc-db3a3c31b388.png', 23, 100, 0)\n"
     ]
    }
   ],
   "source": [
    "new_dataset = StickerDataset(root='reid-data')\n",
    "train_list = new_dataset.train\n",
    "query_list = new_dataset.query\n",
    "gallery_list = new_dataset.gallery\n",
    "\n",
    "# Print the first few entries of each list for verification\n",
    "print(\"Train List:\")\n",
    "for entry in train_list[:5]:\n",
    "    print(entry)\n",
    "\n",
    "print(\"Query List:\")\n",
    "for entry in query_list[:5]:\n",
    "    print(entry)\n",
    "\n",
    "print(\"Gallery List:\")\n",
    "for entry in gallery_list[:5]:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f318b304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'resnet50_fc512', 'se_resnet50', 'se_resnet50_fc512', 'se_resnet101', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'densenet121_fc512', 'inceptionresnetv2', 'inceptionv4', 'xception', 'resnet50_ibn_a', 'resnet50_ibn_b', 'nasnsetmobile', 'mobilenetv2_x1_0', 'mobilenetv2_x1_4', 'shufflenet', 'squeezenet1_0', 'squeezenet1_0_fc512', 'squeezenet1_1', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'mudeep', 'resnet50mid', 'hacnn', 'pcb_p6', 'pcb_p4', 'mlfn', 'osnet_x1_0', 'osnet_x0_75', 'osnet_x0_5', 'osnet_x0_25', 'osnet_ibn_x1_0', 'osnet_ain_x1_0', 'osnet_ain_x0_75', 'osnet_ain_x0_5', 'osnet_ain_x0_25']\n"
     ]
    }
   ],
   "source": [
    "torchreid.models.show_avai_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727808da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "def load_checkpoint(fpath):\n",
    "    r\"\"\"Loads checkpoint.\n",
    "\n",
    "    ``UnicodeDecodeError`` can be well handled, which means\n",
    "    python2-saved files can be read from python3.\n",
    "\n",
    "    Args:\n",
    "        fpath (str): path to checkpoint.\n",
    "\n",
    "    Returns:\n",
    "        dict\n",
    "\n",
    "    Examples::\n",
    "        >>> from torchreid.utils import load_checkpoint\n",
    "        >>> fpath = 'log/my_model/model.pth.tar-10'\n",
    "        >>> checkpoint = load_checkpoint(fpath)\n",
    "    \"\"\"\n",
    "    if fpath is None:\n",
    "        raise ValueError('File path is None')\n",
    "    fpath = osp.abspath(osp.expanduser(fpath))\n",
    "    if not osp.exists(fpath):\n",
    "        raise FileNotFoundError('File is not found at \"{}\"'.format(fpath))\n",
    "    map_location = None if torch.cuda.is_available() else 'cpu'\n",
    "    try:\n",
    "        checkpoint = torch.load(fpath, map_location=map_location, weights_only=False)\n",
    "    except UnicodeDecodeError:\n",
    "        pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
    "        pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
    "        checkpoint = torch.load(\n",
    "            fpath, pickle_module=pickle, map_location=map_location,weights_only=False\n",
    "        )\n",
    "    except Exception:\n",
    "        print('Unable to load checkpoint from \"{}\"'.format(fpath))\n",
    "        raise\n",
    "    return checkpoint\n",
    "\n",
    "def load_pretrained_weights(model, weight_path):\n",
    "    r\"\"\"Loads pretrianed weights to model.\n",
    "\n",
    "    Features::\n",
    "        - Incompatible layers (unmatched in name or size) will be ignored.\n",
    "        - Can automatically deal with keys containing \"module.\".\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): network model.\n",
    "        weight_path (str): path to pretrained weights.\n",
    "\n",
    "    Examples::\n",
    "        >>> from torchreid.utils import load_pretrained_weights\n",
    "        >>> weight_path = 'log/my_model/model-best.pth.tar'\n",
    "        >>> load_pretrained_weights(model, weight_path)\n",
    "    \"\"\"\n",
    "    checkpoint = load_checkpoint(weight_path)\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "    new_state_dict = OrderedDict()\n",
    "    matched_layers, discarded_layers = [], []\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            k = k[7:] # discard module.\n",
    "\n",
    "        if k in model_dict and model_dict[k].size() == v.size():\n",
    "            new_state_dict[k] = v\n",
    "            matched_layers.append(k)\n",
    "        else:\n",
    "            discarded_layers.append(k)\n",
    "\n",
    "    model_dict.update(new_state_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    if len(matched_layers) == 0:\n",
    "        print.warn(\n",
    "            'The pretrained weights \"{}\" cannot be loaded, '\n",
    "            'please check the key names manually '\n",
    "            '(** ignored and continue **)'.format(weight_path)\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            'Successfully loaded pretrained weights from \"{}\"'.\n",
    "            format(weight_path)\n",
    "        )\n",
    "        if len(discarded_layers) > 0:\n",
    "            print(\n",
    "                '** The following layers are discarded '\n",
    "                'due to unmatched keys or layer size: {}'.\n",
    "                format(discarded_layers)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d5048c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(load_pretrained=True):\n",
    "\n",
    "    model = torchreid.models.build_model(\n",
    "                name=\"resnet50\",\n",
    "                num_classes=datamanager.num_train_pids,\n",
    "        ).to(device)\n",
    "    if load_pretrained:\n",
    "        load_pretrained_weights(model, 'log/resnet50/model/model.pth.tar-40')\n",
    "    return model\n",
    "def init(model):\n",
    "        optimizer = torchreid.optim.build_optimizer(\n",
    "                model,\n",
    "        )\n",
    "        scheduler = torchreid.optim.build_lr_scheduler(\n",
    "                optimizer,\n",
    "                lr_scheduler=\"single_step\",\n",
    "                stepsize=20\n",
    "        )\n",
    "\n",
    "        engine = torchreid.engine.ImageSoftmaxEngine(\n",
    "                datamanager,\n",
    "                model,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "        )\n",
    "\n",
    "        return engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2960855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(engine):\n",
    "    engine.run(\n",
    "                save_dir=\"log/resnet50\",\n",
    "                max_epoch=25,\n",
    "                eval_freq=10,\n",
    "                print_freq=10,\n",
    "                test_only=False,\n",
    "                fixbase_epoch=5,\n",
    "                open_layers='classifier',\n",
    "                visrank=False,\n",
    "                visrank_topk=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9dba67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(engine, model):\n",
    "    engine.run(\n",
    "        save_dir=\"log/resnet50\",\n",
    "        test_only=True,\n",
    "        open_layers='classifier',\n",
    "        visrank=False,\n",
    "        visrank_topk=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f157aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded pretrained weights from \"log/resnet50/model/model.pth.tar-40\"\n",
      "##### Evaluating sticker_dataset (source) #####\n",
      "Extracting features from query set ...\n",
      "Done, obtained 42-by-2048 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 30-by-2048 matrix\n",
      "Speed: 0.0507 sec/batch\n",
      "Computing distance matrix with metric=euclidean ...\n",
      "Computing CMC and mAP ...\n",
      "Note: number of gallery samples is quite small, got 30\n",
      "** Results **\n",
      "mAP: 37.4%\n",
      "CMC curve\n",
      "Rank-1  : 26.7%\n",
      "Rank-5  : 46.7%\n",
      "Rank-10 : 63.3%\n",
      "Rank-20 : 86.7%\n"
     ]
    }
   ],
   "source": [
    "model = load_model(load_pretrained=True)\n",
    "engine = init(model)\n",
    "# train(engine)\n",
    "test(engine, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b2392",
   "metadata": {},
   "source": [
    "## Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d0bbd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model/model_1744450299.pth\n",
      "Mapping information saved to model/model_1744450299.json\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "base_path = 'model'\n",
    "dataset = datamanager.train_loader.dataset\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "timestamp = int(time.time())\n",
    "\n",
    "# Save model state dict\n",
    "model_state_path = f'{base_path}/model_{timestamp}.pth'\n",
    "torch.save(model.state_dict(), model_state_path)\n",
    "\n",
    "# Save mapping information\n",
    "model_data = {\n",
    "    'group_id_to_pid': dataset.group_id_to_pid,\n",
    "    'group_name_to_pid': dataset.pid_to_group_name,\n",
    "    'model_info': {\n",
    "        'timestamp': timestamp,\n",
    "        'input_size': model.input_size if hasattr(model, 'input_size') else None,\n",
    "        'output_size': model.output_size if hasattr(model, 'output_size') else None,\n",
    "        'model_type': model.__class__.__name__\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "json_path = model_state_path.replace('.pth', '.json')\n",
    "with open(json_path, 'w') as json_file:\n",
    "    json.dump(model_data, json_file)\n",
    "\n",
    "print(f\"Model saved to {model_state_path}\")\n",
    "print(f\"Mapping information saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb3a09",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model/model_1744379168.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     12\u001b[39m model2 = torchreid.models.build_model(\n\u001b[32m     13\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mresnet50\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     14\u001b[39m     num_classes=\u001b[38;5;28mlen\u001b[39m(group_id_to_pid),\n\u001b[32m     15\u001b[39m ).to(device)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Load model state\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m model2.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m model2.to(device)\n\u001b[32m     20\u001b[39m model2.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/stick-it-ai/venv/lib64/python3.13/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/stick-it-ai/venv/lib64/python3.13/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/stick-it-ai/venv/lib64/python3.13/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'model/model_1744379168.pth'"
     ]
    }
   ],
   "source": [
    "model_path = 'model/model_1744379168.pth'\n",
    "if json_path is None:\n",
    "    json_path = model_path.replace('.pth', '.json')\n",
    "\n",
    "# Load mapping information\n",
    "with open(json_path, 'r') as json_file:\n",
    "    model_data = json.load(json_file)\n",
    "\n",
    "group_id_to_pid = model_data['group_id_to_pid']\n",
    "\n",
    "\n",
    "model2 = torchreid.models.build_model(\n",
    "    name=\"resnet50\", \n",
    "    num_classes=len(group_id_to_pid),\n",
    ").to(device)\n",
    "\n",
    "# Load model state\n",
    "model2.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model2.to(device)\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d6b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
